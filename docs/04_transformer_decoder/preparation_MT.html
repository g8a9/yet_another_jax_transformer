
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Preparation for the MT task &#8212; Yet Another (JAX) Transformer</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Training a Neural Machine Translation Model üá¨üáß -&gt; üáÆüáπ" href="training_MT.html" />
    <link rel="prev" title="The Transformer Decoder" href="decoder.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-NWJV81P0BZ"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-NWJV81P0BZ');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Yet Another (JAX) Transformer</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Yet Another (JAX) Transformer
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01_introduction_to_the_architecture/intro.html">
   1Ô∏è‚É£ Introduction to The Transformer Architecture
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02_core_components/intro.html">
   2Ô∏è‚É£ Implementing the Core Components
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02_core_components/attention.html">
     Attention Mechanism in the Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02_core_components/multi_headed_attention.html">
     The Multi-Headed Attention
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02_core_components/embedding_positional.html">
     Turning Tokens into Vectors: Embeddings and Positional Encoding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03_transformer_encoder/intro.html">
   3Ô∏è‚É£ Transformer Encoder and Word-level Language Modeling
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03_transformer_encoder/encoder.html">
     Combining all together: the Transformer Encoder
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03_transformer_encoder/language_modeling.html">
     üöÄ Training your First Language Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03_transformer_encoder/sentiment_analysis.html">
     Fine-Tuning for Sentiment Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="intro.html">
   5Ô∏è‚É£ Transformer and Neural Machine Translation
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="decoder.html">
     The Transformer Decoder
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Preparation for the MT task
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="training_MT.html">
     Training a Neural Machine Translation Model üá¨üáß -&gt; üáÆüáπ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="quantitative_evaluation.html">
     Quantitative Evaluation with BLEU
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05_gender_bias_in_MT/intro.html">
   (
   <em>
    Bonus
   </em>
   )¬†All the glitter is not gold: Gender Bias in Machine Translation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05_gender_bias_in_MT/representational_harm.html">
     Assessing Representational Harm using WinoMT
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/g8a9/Yet Another (JAX) Transformer"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/g8a9/Yet Another (JAX) Transformer/issues/new?title=Issue%20on%20page%20%2Fdocs/04_transformer_decoder/preparation_MT.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/docs/04_transformer_decoder/preparation_MT.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset-selection">
   Dataset selection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-tokenizer-for-the-machine-translation-task">
   Train tokenizer for the Machine Translation task
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#process-and-tokenize-mt-data">
   Process and tokenize MT data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utility-functions-and-data-structures">
   Utility functions and data structures
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Preparation for the MT task</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset-selection">
   Dataset selection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-tokenizer-for-the-machine-translation-task">
   Train tokenizer for the Machine Translation task
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#process-and-tokenize-mt-data">
   Process and tokenize MT data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utility-functions-and-data-structures">
   Utility functions and data structures
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="preparation-for-the-mt-task">
<h1>Preparation for the MT task<a class="headerlink" href="#preparation-for-the-mt-task" title="Permalink to this headline">#</a></h1>
<p>With the Transformer class ready, we can now take our time to go through all the required steps in preparation for the actual training. As you have already seen in Section 3Ô∏è‚É£, we mainly need to:</p>
<ol class="simple">
<li><p>pick a dataset. We need a parallel corpus, where each sample is made of a source and a target sentence;</p></li>
<li><p>train a tokenizer;</p></li>
<li><p>preprocess our corpus using the tokenizer.</p></li>
</ol>
<section id="dataset-selection">
<h2>Dataset selection<a class="headerlink" href="#dataset-selection" title="Permalink to this headline">#</a></h2>
<p>We will use two well-known datasets to train an English-to-Italian translation system.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://opus.nlpl.eu/Tatoeba.php">TatoEBA</a> is a crowdsourced dataset of sentences annotated on the homonym <a class="reference external" href="https://tatoeba.org/en/">website</a> by users;</p></li>
<li><p><a class="reference external" href="https://www.statmt.org/europarl/">Europarl</a> is a corpus of proceedings of the European Parliament.</p></li>
</ul>
<p>We demonstrate the training and provide a few translation examples on TatoEBA since it is smaller and easier to train on. However, you can also download the Europarl dataset by executing the cell below and proceed equivalently (depending on your computing capacity, training on Europarl will be feasible or not).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#¬†Skip this if you are running on Colab or on a low-end GPU or CPU.</span>
<span class="n">raw_datasets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;g8a9/europarl_en-it&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="train-tokenizer-for-the-machine-translation-task">
<h2>Train tokenizer for the Machine Translation task<a class="headerlink" href="#train-tokenizer-for-the-machine-translation-task" title="Permalink to this headline">#</a></h2>
<p>Here, we opt for training a single tokenizer with double the number of tokens stored compared to the one used for language modeling.</p>
<p>Feel free to test your solution with two different tokenizers (one per language).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Target tokenizer (SRC+TGT language)</span>
<span class="n">VOCAB_SIZE</span> <span class="o">=</span> <span class="mi">20_000</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">NUM_LAYERS</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">NUM_HEADS</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">D_MODEL</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">D_FF</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">P_DROPOUT</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">3e-4</span>
<span class="n">GRAD_CLIP_VALUE</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Loading TatoEBA</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s2">&quot;it-en.tsv&quot;</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;id_it&quot;</span><span class="p">,</span> <span class="s2">&quot;sent_it&quot;</span><span class="p">,</span> <span class="s2">&quot;id_en&quot;</span><span class="p">,</span> <span class="s2">&quot;sent_en&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># we will use italian sentences to generate our target tokenizer</span>
<span class="n">it_sentences</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;sent_it&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">()</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">en_sentences</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;sent_en&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">()</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unique Italian sentences: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">it_sentences</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Samples:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">it_sentences</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>

<span class="c1"># we&#39;ll use BPE</span>
<span class="n">mt_tokenizer</span> <span class="o">=</span> <span class="n">tokenizers</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">(</span><span class="n">tokenizers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">BPE</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">))</span>
<span class="n">mt_tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">tokenizers</span><span class="o">.</span><span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Whitespace</span><span class="p">()</span>
<span class="n">mt_tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">tokenizers</span><span class="o">.</span><span class="n">normalizers</span><span class="o">.</span><span class="n">Lowercase</span><span class="p">()</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">tokenizers</span><span class="o">.</span><span class="n">trainers</span><span class="o">.</span><span class="n">BpeTrainer</span><span class="p">(</span>
    <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[BOS]&quot;</span><span class="p">,</span> <span class="s2">&quot;[EOS]&quot;</span><span class="p">],</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">VOCAB_SIZE</span><span class="p">,</span>
    <span class="n">show_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">min_frequency</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">continuing_subword_prefix</span><span class="o">=</span><span class="s2">&quot;##&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">mt_tokenizer</span><span class="o">.</span><span class="n">train_from_iterator</span><span class="p">(</span>
    <span class="n">it_sentences</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="n">en_sentences</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span>
<span class="p">)</span>

<span class="n">bos_id</span><span class="p">,</span> <span class="n">eos_id</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">mt_tokenizer</span><span class="o">.</span><span class="n">token_to_id</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;[BOS]&quot;</span><span class="p">,</span> <span class="s2">&quot;[EOS]&quot;</span><span class="p">])</span>
<span class="n">mt_tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">tokenizers</span><span class="o">.</span><span class="n">processors</span><span class="o">.</span><span class="n">BertProcessing</span><span class="p">(</span>
    <span class="p">(</span><span class="s2">&quot;[EOS]&quot;</span><span class="p">,</span> <span class="n">eos_id</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;[BOS]&quot;</span><span class="p">,</span> <span class="n">bos_id</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">mt_tokenizer</span><span class="o">.</span><span class="n">enable_truncation</span><span class="p">(</span><span class="n">MAX_SEQ_LEN</span><span class="p">)</span>
<span class="n">mt_tokenizer</span><span class="o">.</span><span class="n">enable_padding</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="n">MAX_SEQ_LEN</span><span class="p">)</span>

<span class="n">PAD_ID</span> <span class="o">=</span> <span class="n">mt_tokenizer</span><span class="o">.</span><span class="n">token_to_id</span><span class="p">(</span><span class="s2">&quot;[PAD]&quot;</span><span class="p">)</span>

<span class="n">mt_tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;mt_tokenizer.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Use the cell below if you want instead to load the tokenizer from disk.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mt_tokenizer</span> <span class="o">=</span> <span class="n">tokenizers</span><span class="o">.</span><span class="n">Tokenizer</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span><span class="s2">&quot;mt_tokenizer.json&quot;</span><span class="p">)</span>
<span class="n">mt_tokenizer</span><span class="o">.</span><span class="n">enable_truncation</span><span class="p">(</span><span class="n">MAX_SEQ_LEN</span><span class="p">)</span>
<span class="n">mt_tokenizer</span><span class="o">.</span><span class="n">enable_padding</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="n">MAX_SEQ_LEN</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="process-and-tokenize-mt-data">
<h2>Process and tokenize MT data<a class="headerlink" href="#process-and-tokenize-mt-data" title="Permalink to this headline">#</a></h2>
<p>As it is not the tutorial‚Äôs focus, we again provide the code to run the basic preprocessing using <code class="docutils literal notranslate"><span class="pre">datasets</span></code>. Feel free to inspect to understand better every step related to tokenization and data preparation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DATASET_SAMPLE</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># @param {type:&quot;number&quot;}</span>

<span class="c1"># generate parallel data</span>
<span class="n">mt_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="n">DATASET_SAMPLE</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">train_df_mt</span><span class="p">,</span> <span class="n">test_df_mt</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">mt_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">val_df_mt</span><span class="p">,</span> <span class="n">test_df_mt</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">test_df_mt</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train&quot;</span><span class="p">,</span> <span class="n">train_df_mt</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;Valid&quot;</span><span class="p">,</span> <span class="n">val_df_mt</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;Test&quot;</span><span class="p">,</span> <span class="n">test_df_mt</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">raw_datasets</span> <span class="o">=</span> <span class="n">DatasetDict</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;sent_en&quot;</span><span class="p">:</span> <span class="n">train_df_mt</span><span class="p">[</span><span class="s2">&quot;sent_en&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="s2">&quot;sent_it&quot;</span><span class="p">:</span> <span class="n">train_df_mt</span><span class="p">[</span><span class="s2">&quot;sent_it&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
            <span class="p">}</span>
        <span class="p">),</span>
        <span class="s2">&quot;valid&quot;</span><span class="p">:</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;sent_en&quot;</span><span class="p">:</span> <span class="n">val_df_mt</span><span class="p">[</span><span class="s2">&quot;sent_en&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="s2">&quot;sent_it&quot;</span><span class="p">:</span> <span class="n">val_df_mt</span><span class="p">[</span><span class="s2">&quot;sent_it&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
            <span class="p">}</span>
        <span class="p">),</span>
        <span class="s2">&quot;test&quot;</span><span class="p">:</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;sent_en&quot;</span><span class="p">:</span> <span class="n">test_df_mt</span><span class="p">[</span><span class="s2">&quot;sent_en&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="s2">&quot;sent_it&quot;</span><span class="p">:</span> <span class="n">test_df_mt</span><span class="p">[</span><span class="s2">&quot;sent_it&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
            <span class="p">}</span>
        <span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">examples</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">mt_tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;sent_en&quot;</span><span class="p">],</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">tgt</span> <span class="o">=</span> <span class="n">mt_tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;sent_it&quot;</span><span class="p">],</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;src_ids&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">ids</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">src</span><span class="p">],</span>
        <span class="s2">&quot;src_mask&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">attention_mask</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">src</span><span class="p">],</span>
        <span class="s2">&quot;src_special_tokens_mask&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">special_tokens_mask</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">src</span><span class="p">],</span>
        <span class="s2">&quot;tgt_ids&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">ids</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">tgt</span><span class="p">],</span>
    <span class="p">}</span>


<span class="n">proc_datasets</span> <span class="o">=</span> <span class="n">raw_datasets</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="n">preprocess</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sent_en&quot;</span><span class="p">,</span> <span class="s2">&quot;sent_it&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First training sample, after processing:&quot;</span><span class="p">,</span> <span class="n">proc_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="utility-functions-and-data-structures">
<h2>Utility functions and data structures<a class="headerlink" href="#utility-functions-and-data-structures" title="Permalink to this headline">#</a></h2>
<p>Let‚Äôs define a few functions that will be useful later.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">subsequent_mask</span><span class="p">(</span><span class="n">S</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Mask out subsequent positions.</span>
<span class="sd">    </span>
<span class="sd">    Given an integer `S`, generate a `1xSxS` matrix containing the attention mask to apply to the sequence.</span>
<span class="sd">    The matrix should implement autoregressive attention (left-context attention), i.e., it should mask, for each token at position &#39;i&#39;, every token in [0, &#39;i&#39;-1).</span>
<span class="sd">    </span>
<span class="sd">    E.g. </span>

<span class="sd">    MAX_LEN = 8</span>
<span class="sd">    SEQ_LEN = 5</span>

<span class="sd">    Encoder attention mask:</span>

<span class="sd">    [ [1, 1, 1, 1, 1, 0, 0, 0, ]</span>
<span class="sd">    [1, 1, 1, 1, 1, 0, 0, 0, ]</span>
<span class="sd">    [1, 1, 1, 1, 1, 0, 0, 0, ]</span>
<span class="sd">    [1, 1, 1, 1, 1, 0, 0, 0, ]</span>
<span class="sd">    [1, 1, 1, 1, 1, 0, 0, 0, ]</span>
<span class="sd">    [1, 1, 1, 1, 1, 0, 0, 0, ]</span>
<span class="sd">    [1, 1, 1, 1, 1, 0, 0, 0, ]</span>
<span class="sd">    [1, 1, 1, 1, 1, 0, 0, 0, ] ]</span>

<span class="sd">    Decoder attention mask:</span>

<span class="sd">    [ [1, 0, 0, 0, 0, 0, 0, 0, ]</span>
<span class="sd">    [1, 1, 0, 0, 0, 0, 0, 0, ]</span>
<span class="sd">    [1, 1, 1, 0, 0, 0, 0, 0, ]</span>
<span class="sd">    [1, 1, 1, 1, 0, 0, 0, 0, ]</span>
<span class="sd">    [1, 1, 1, 1, 1, 0, 0, 0, ]</span>
<span class="sd">    [1, 1, 1, 1, 1, 1, 0, 0, ]</span>
<span class="sd">    [1, 1, 1, 1, 1, 1, 1, 0, ]</span>
<span class="sd">    [1, 1, 1, 1, 1, 1, 1, 1, ] ]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">attn_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>
    <span class="n">subsequent_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">attn_shape</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">subsequent_mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">collate_fn_mt</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Collate source and target sequences in the batch.</span>

<span class="sd">    We also need to define a &#39;labels&#39; variable.</span>

<span class="sd">    You want to produce the following shapes:</span>
<span class="sd">    - src: (B,MAX_SEQ_LEN)</span>
<span class="sd">    - src_mask: (B,1,MAX_SEQ_LEN)</span>
<span class="sd">    - tgt: (B,MAX_SEQ_LEN-1)</span>
<span class="sd">    - tgt_mask: (B,MAX_SEQ_LEN-1,MAX_SEQ_LEN-1)</span>
<span class="sd">    - labels: (B,MAX_SEQ_LEN-1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="s2">&quot;src_ids&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
    <span class="n">src_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="s2">&quot;src_mask&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
    <span class="n">src_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">src_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">tgt_seq</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="s2">&quot;tgt_ids&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
    <span class="n">tgt</span> <span class="o">=</span> <span class="n">tgt_seq</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># (B,MAX_SEQ_LEN-1)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">tgt_seq</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>  <span class="c1"># (B,MAX_SEQ_LEN-1)</span>

    <span class="n">tgt_pad</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tgt</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="n">PAD_ID</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tgt_pad</span> <span class="o">&amp;</span> <span class="n">subsequent_mask</span><span class="p">(</span><span class="n">tgt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">item</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;src&quot;</span><span class="p">:</span> <span class="n">src</span><span class="p">,</span>
        <span class="s2">&quot;src_mask&quot;</span><span class="p">:</span> <span class="n">src_mask</span><span class="p">,</span>
        <span class="s2">&quot;tgt&quot;</span><span class="p">:</span> <span class="n">tgt</span><span class="p">,</span>
        <span class="s2">&quot;tgt_mask&quot;</span><span class="p">:</span> <span class="n">tgt_mask</span><span class="p">,</span>
        <span class="s2">&quot;labels&quot;</span><span class="p">:</span> <span class="n">labels</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">item</span>


<span class="n">train_loader_mt</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">proc_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn_mt</span>
<span class="p">)</span>
<span class="n">valid_loader_mt</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">proc_datasets</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn_mt</span>
<span class="p">)</span>
<span class="n">test_loader_mt</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">proc_datasets</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn_mt</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Batches Train: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader_mt</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;Valid: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_loader_mt</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;Test : </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_loader_mt</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/04_transformer_decoder"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="decoder.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">The Transformer Decoder</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="training_MT.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Training a Neural Machine Translation Model üá¨üáß -&gt; üáÆüáπ</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Giuseppe Attanasio, Moreno La Quatra<br/>
  
      &copy; Copyright 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>